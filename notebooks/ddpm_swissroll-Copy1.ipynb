{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4f2046",
   "metadata": {},
   "source": [
    "# Denoising diffusion: 2D Swiss roll\n",
    "\n",
    "Generative modeling with DDPMs is demonstrated on the basis of a toy example. A 2D version of the good old Swiss roll data set is used to train a simple denoising diffusion model. The example merely serves the purpose of quickly familiarizing with the algorithm and its properties. Most of the model architecture or hyperparameter choices are quite arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e025d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from diffusion import (\n",
    "    DDPM,\n",
    "    ConditionalDenseModel,\n",
    "    make_beta_schedule\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e6e76",
   "metadata": {},
   "source": [
    "## Swiss roll data\n",
    "\n",
    "Synthetic training and validation sets are generated in the following, totalling $N=2000$ samples $\\{\\boldsymbol{x}_{0,i}\\}_{i=1}^N$. They are created with function `make_swiss_roll` from scikit-learn. The data are restricted to two dimensions and scaled. Any further normalization will be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ba85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 2000\n",
    "noise_level = 0.5\n",
    "\n",
    "X, _ = make_swiss_roll(num_samples, noise=noise_level)\n",
    "X = X[:,[0,2]] / 10 # restrict to 2D and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e923e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.scatter(X[:,0], X[:,1], s=20, alpha=0.7, edgecolors='none')\n",
    "ax.set(xlim=(-2, 2.25), ylim=(-2, 2))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.grid(visible=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ecd89",
   "metadata": {},
   "source": [
    "After the data have been generated, PyTorch `TensorDataset`s and `DataLoader`s are constructed. They allow for assembling and accessing mini-batches of data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59097757",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(X, test_size=0.2)\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "X_val = torch.tensor(X_val).float()\n",
    "\n",
    "train_set = TensorDataset(X_train)\n",
    "val_set = TensorDataset(X_val)\n",
    "\n",
    "print('No. train images:', len(train_set))\n",
    "print('No. val. images:', len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c43df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print('No. train batches:', len(train_loader))\n",
    "print('No. val. batches:', len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de1793",
   "metadata": {},
   "source": [
    "## DDPM\n",
    "\n",
    "\n",
    "An instance of `ConditionalDenseModel` is employed as the model $\\boldsymbol{\\epsilon}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t)$. It is composed of a number of fully connected layers and ReLU activation functions. Every layer is explicitly conditioned on the time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2484b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [2, 128, 128, 128, 2]\n",
    "\n",
    "eps_model = ConditionalDenseModel(\n",
    "    num_features,\n",
    "    activation='relu',\n",
    "    embed_dim=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0426c45",
   "metadata": {},
   "source": [
    "Our diffusion process consists of $T=200$ time steps. Note that, very probably, one could even get away with a smaller number. The corresponding $\\beta$-schedule, which represents an important setting, is created with `make_beta_schedule`. A `DDPM`-object is initialized, providing methods for the forward and reverse process. Moreover, it allows for computing the simplified stochastic optimization objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# betas = make_beta_schedule(num_steps=500, mode='linear', beta_range=(1e-04, 0.02))\n",
    "# betas = make_beta_schedule(num_steps=500, mode='cosine', cosine_s=0.008)\n",
    "betas = make_beta_schedule(num_steps=500, mode='sigmoid', sigmoid_range=(-5, 5))\n",
    "\n",
    "ddpm = DDPM(\n",
    "    eps_model=eps_model,\n",
    "    betas=betas,\n",
    "    criterion='mse',\n",
    "    lr=1e-03\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca369501",
   "metadata": {},
   "source": [
    "The values $\\beta_t$ and $\\sqrt{\\bar{\\alpha}_t}$ as well as the log-SNR corresponding to the selected schedule are plotted below for all $t=1,\\ldots,200$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4fa0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(9, 2.5))\n",
    "\n",
    "ax1.plot(np.arange(len(ddpm.betas)) + 1, ddpm.betas)\n",
    "ax1.set(xlim=(0, len(ddpm.betas)), ylim=(0, ddpm.betas.max()))\n",
    "ax1.set(xlabel='t', ylabel='$\\\\beta$')\n",
    "ax1.grid(visible=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax1.set_axisbelow(True)\n",
    "\n",
    "ax2.plot(np.arange(len(ddpm.alphas_bar)) + 1, ddpm.alphas_bar.sqrt())\n",
    "ax2.set(xlim=(0, len(ddpm.alphas_bar)), ylim=(0, 1))\n",
    "ax2.set(xlabel='t', ylabel='$\\\\sqrt{\\\\bar{\\\\alpha}}$')\n",
    "ax2.grid(visible=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax2.set_axisbelow(True)\n",
    "\n",
    "ax3.plot(np.arange(len(ddpm.alphas_bar)) + 1, torch.log(ddpm.alphas_bar / (1 - ddpm.alphas_bar)))\n",
    "ax3.set(xlim=(0, len(ddpm.alphas_bar)))\n",
    "ax3.set(xlabel='t', ylabel='log-SNR')\n",
    "ax3.grid(visible=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax3.set_axisbelow(True)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f2a03",
   "metadata": {},
   "source": [
    "## Forward process simulation\n",
    "\n",
    "It is interesting to visualize the forward diffusion process applied to the problem at hand. To that end, the training data are diffused step by step according to $q(\\boldsymbol{x}_t | \\boldsymbol{x}_{t-1}) = \\mathcal{N}(\\boldsymbol{x}_t | \\sqrt{1-\\beta_t} \\boldsymbol{x}_{t-1}, \\beta_t \\boldsymbol{I})$ with the method `diffuse_all_steps`. Intermediate results for certain time steps can then be plotted. This allows us to observe the advancing diffusion process from data to pure noise \"in action\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d01456",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_noisy = ddpm.diffuse_all_steps(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8798f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_steps = [0, 10, 20, 50, 100, 500]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(plot_steps), figsize=(9, 2))\n",
    "for time_idx, ax in zip(plot_steps, axes.ravel()):\n",
    "    samples = x_noisy[time_idx].numpy()\n",
    "    ax.scatter(samples[:,0], samples[:,1], s=10, alpha=0.7, edgecolors='none')\n",
    "    ax.set(xlim=(-2.25, 2.5), ylim=(-2.25, 2.25))\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_title('{} steps'.format(time_idx))\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd51974c",
   "metadata": {},
   "source": [
    "It is noted that the simulation of $q(\\boldsymbol{x}_t | \\boldsymbol{x}_0) = \\mathcal{N}(\\boldsymbol{x}_t | \\sqrt{\\bar{\\alpha}_t} \\boldsymbol{x}_0, (1-\\bar{\\alpha}_t) \\boldsymbol{I})$ at any time step directly is enabled by the method `diffuse`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66419f",
   "metadata": {},
   "source": [
    "## Reverse process training\n",
    "\n",
    "We are eventually all set for training the model specified above. The loss $L_\\text{simple} = \\mathbb{E}_{\\mathcal{U}(t|1, T), q(\\boldsymbol{x}_0), \\mathcal{N}(\\boldsymbol{\\epsilon} | \\boldsymbol{0}, \\boldsymbol{I})}[\\lVert \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\boldsymbol{\\theta}(\\sqrt{\\bar{\\alpha}_t} \\boldsymbol{x}_0 + \\sqrt{1-\\bar{\\alpha}_t} \\boldsymbol{\\epsilon}, t) \\rVert^2]$ has to be minimized for that purpose. This is accomplished with PyTorch Lightning, a library conveniently performing training, logging and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2887fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl.loggers.CSVLogger('.', name='lightning_logs', version=0)\n",
    "\n",
    "trainer = pl.Trainer(logger=logger,\n",
    "                     max_epochs=1000,\n",
    "                     log_every_n_steps=len(train_loader),\n",
    "                     enable_progress_bar=False)\n",
    "\n",
    "trainer.validate(ddpm, dataloaders=val_loader, verbose=False) # check validation loss before training\n",
    "trainer.fit(ddpm, train_dataloaders=train_loader, val_dataloaders=val_loader) # start training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d2c74f",
   "metadata": {},
   "source": [
    "Note the extreme noise in the train and validation curves below. In addition to the inevitably imperfect and noisy convergence behavior, and the mini-batched evaluation of the training loss, the stochastic loss function here contributes to the overall noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_csv('lightning_logs/version_0/metrics.csv')\n",
    "\n",
    "train_df = metrics_df[['step', 'train_loss']].dropna()\n",
    "val_df = metrics_df[['step', 'val_loss']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(train_df['step'], train_df['train_loss'], alpha=0.7, label='train')\n",
    "ax.plot(val_df['step'], val_df['val_loss'], alpha=0.7, label='val')\n",
    "ax.set(xlabel='step', ylabel='loss')\n",
    "ax.set_xlim([0, max(train_df['step'].max(), val_df['step'].max())])\n",
    "ax.legend()\n",
    "ax.grid(visible=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da5fa56",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484490f",
   "metadata": {},
   "source": [
    "After having trained the model, the denoising process can be used for data generation. The `denoise_all_steps` method allows one to perform $p_\\boldsymbol{\\theta}(\\boldsymbol{x}_{t-1} | \\boldsymbol{x}_t) = \\mathcal{N}(\\boldsymbol{x}_{t-1} | \\boldsymbol{\\mu}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t), \\sigma_t^2 \\boldsymbol{I})$ with $\\boldsymbol{\\mu}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}}(\\boldsymbol{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_\\boldsymbol{\\theta}(\\boldsymbol{x}_t, t))$ in step-wise fashion. This way, the progressive generation can be visualized for intermediate steps. The evolution from pure noise into the targeted spiral-shaped structure can be observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm.eval()\n",
    "x_denoise = ddpm.denoise_all_steps(torch.randn(1000, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fba144",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_steps_reverse = [ddpm.num_steps - s for s in reversed(plot_steps)]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(plot_steps_reverse), figsize=(9, 2))\n",
    "for time_idx, ax in zip(plot_steps_reverse, axes.ravel()):\n",
    "    samples = x_denoise[time_idx].numpy()\n",
    "    ax.scatter(samples[:,0], samples[:,1], s=10, alpha=0.7, edgecolors='none')\n",
    "    ax.set(xlim=(-2.25, 2.5), ylim=(-2.25, 2.25))\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_title('{} steps'.format(time_idx))\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d359e",
   "metadata": {},
   "source": [
    "The same process is also implemented by `generate`, without storing intermediate results though. We use this method to generate some final samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222bd43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gen = ddpm.generate(sample_shape=(2,), num_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41722b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ax.scatter(x_gen[:,0], x_gen[:,1], s=20, alpha=0.7, edgecolors='none')\n",
    "ax.set(xlim=(-2, 2.25), ylim=(-2, 2))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.grid(visible=True, which='both', color='gray', alpha=0.2, linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e578aa",
   "metadata": {},
   "source": [
    "Well, it seems to work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
